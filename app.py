from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from typing import Optional
import uvicorn

# ===============================
# FastAPI ì•± ì´ˆê¸°í™”
# ===============================
app = FastAPI(
    title="Gemma íŒŒì¸íŠœë‹ ëª¨ë¸ API",
    description="íŒŒì¸íŠœë‹ëœ Gemma ëª¨ë¸ì„ í†µí•´ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” API",
    version="1.0.0"
)

# ===============================
# ì „ì—­ ë³€ìˆ˜ (ëª¨ë¸ ë¡œë“œ)
# ===============================
MODEL_PATH = "./gemma2-finetuned"
tokenizer = None
model = None
device = "cpu"  # Hugging Face SpacesëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPU

# ===============================
# ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ì •ì˜
# ===============================
class QuestionRequest(BaseModel):
    question: str
    max_tokens: Optional[int] = 120
    temperature: Optional[float] = 0.3
    top_k: Optional[int] = 40
    top_p: Optional[float] = 0.8
    repetition_penalty: Optional[float] = 1.8
    
    class Config:
        json_schema_extra = {
            "example": {
                "question": "ì˜¤ë‹µë…¸íŠ¸ëŠ” ì–´ë–»ê²Œ ì •ë¦¬í• ê¹Œìš”?",
                "max_tokens": 120,
                "temperature": 0.3
            }
        }

class AnswerResponse(BaseModel):
    question: str
    answer: str
    full_output: str
    model_path: str
    device: str

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    device: str

# ===============================
# ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
# ===============================
@app.on_event("startup")
async def load_model():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ"""
    global tokenizer, model
    
    print("ğŸš€ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    try:
        # í•„ìš”í•œ ëª¨ë“ˆë“¤ import
        from peft import PeftModel
        import os
        
        # Hugging Face í† í° ê°€ì ¸ì˜¤ê¸° (Space Secretì—ì„œ)
        hf_token = os.environ.get("HF_TOKEN", None)
        
        if hf_token:
            print("âœ… Hugging Face í† í° í™•ì¸ë¨")
        else:
            print("âš ï¸ HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ (ê³µê°œ ëª¨ë¸ë§Œ ì‚¬ìš© ê°€ëŠ¥)")
        
        # ë² ì´ìŠ¤ ëª¨ë¸ê³¼ LoRA adapterë¥¼ ë¶„ë¦¬í•´ì„œ ë¡œë“œ
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (Docker í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
        cache_dir = os.environ.get("HF_HOME", "/tmp/hf_cache")
        print(f"   ğŸ“ ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
        
        # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (google/gemma-2b) - CPU ìµœì í™”
        print("   ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        base_model = AutoModelForCausalLM.from_pretrained(
            "google/gemma-2b",
            torch_dtype=torch.float32,  # CPUì—ì„œëŠ” float32 ì‚¬ìš©
            device_map="cpu",           # CPUë¡œ ê°•ì œ ì„¤ì •
            token=hf_token,
            cache_dir=cache_dir,
            low_cpu_mem_usage=True,     # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
            trust_remote_code=True      # Gemma ëª¨ë¸ í˜¸í™˜ì„±
        )
        
        # 2. LoRA adapter ë¡œë“œ
        print("   ğŸ“¥ LoRA adapter ë¡œë“œ ì¤‘...")
        model = PeftModel.from_pretrained(
            base_model, 
            MODEL_PATH,
            device_map="cpu"  # CPUë¡œ ê°•ì œ ì„¤ì •
        )
        
        # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ (ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë™ì¼)
        tokenizer = AutoTokenizer.from_pretrained(
            "google/gemma-2b",
            token=hf_token,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        model = model.to(device)
        model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (Device: {device})")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

# ===============================
# API ì—”ë“œí¬ì¸íŠ¸
# ===============================

@app.get("/", response_model=dict)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - API ì •ë³´"""
    return {
        "message": "Gemma íŒŒì¸íŠœë‹ ëª¨ë¸ API ì„œë²„",
        "version": "1.0.0",
        "description": "ìˆ˜í—˜ìƒ Q&A ë‹µë³€ ëª¨ë¸",
        "endpoints": {
            "health": "GET /health - ì„œë²„ ìƒíƒœ í™•ì¸",
            "predict": "POST /predict - ì§ˆë¬¸ì— ë‹µë³€",
            "docs": "GET /docs - API ë¬¸ì„œ"
        },
        "usage": {
            "example": {
                "method": "POST",
                "url": "/predict",
                "body": {
                    "question": "ì˜¤ë‹µë…¸íŠ¸ëŠ” ì–´ë–»ê²Œ ì •ë¦¬í• ê¹Œìš”?"
                }
            }
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return HealthResponse(
        status="healthy" if model is not None else "model_not_loaded",
        model_loaded=model is not None,
        device=device
    )

@app.post("/predict", response_model=AnswerResponse)
async def predict(request: QuestionRequest):
    """
    ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
    
    - **question**: ë‹µë³€ì„ ì›í•˜ëŠ” ì§ˆë¬¸
    - **max_tokens**: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 120)
    - **temperature**: ìƒì„± ë‹¤ì–‘ì„± ì¡°ì ˆ (ê¸°ë³¸ê°’: 0.3, ë‚®ì„ìˆ˜ë¡ ë³´ìˆ˜ì )
    - **top_k**: ìƒìœ„ kê°œ í† í°ë§Œ ê³ ë ¤ (ê¸°ë³¸ê°’: 40)
    - **top_p**: nucleus sampling í™•ë¥  (ê¸°ë³¸ê°’: 0.8)
    - **repetition_penalty**: ë°˜ë³µ ë°©ì§€ ê°•ë„ (ê¸°ë³¸ê°’: 1.8)
    """
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"ì§ˆë¬¸: {request.question}\në‹µë³€:"
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt")
        inputs = inputs.to(device)
        
        # ë‹µë³€ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_k=request.top_k,
                top_p=request.top_p,
                do_sample=True,
                repetition_penalty=request.repetition_penalty,
                no_repeat_ngram_size=4,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        answer_only = full_output
        if "ë‹µë³€:" in full_output:
            answer_only = full_output.split("ë‹µë³€:", 1)[1].strip()
            
            # ë‹¤ìŒ ì§ˆë¬¸ì´ ë‚˜ì˜¤ë©´ ê·¸ ì „ê¹Œì§€ë§Œ ì¶”ì¶œ
            if "ì§ˆë¬¸:" in answer_only:
                answer_only = answer_only.split("ì§ˆë¬¸:", 1)[0].strip()
            
            # HTML íƒœê·¸ë‚˜ ì´ìƒí•œ ë¬¸ì ì œê±°
            if "<" in answer_only:
                answer_only = answer_only.split("<", 1)[0].strip()
        
        return AnswerResponse(
            question=request.question,
            answer=answer_only,
            full_output=full_output,
            model_path=MODEL_PATH,
            device=device
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ===============================
# ì„œë²„ ì‹¤í–‰
# ===============================
if __name__ == "__main__":
    print("="*70)
    print("ğŸš€ Gemma íŒŒì¸íŠœë‹ ëª¨ë¸ API ì„œë²„ ì‹œì‘")
    print("="*70)
    print(f"ğŸ“ ì„œë²„ ì£¼ì†Œ: http://0.0.0.0:7860")
    print(f"ğŸ“š API ë¬¸ì„œ: http://0.0.0.0:7860/docs")
    print(f"ğŸ”§ Device: {device}")
    print("="*70)
    
    uvicorn.run(
        app, 
        host="0.0.0.0",
        port=7860,  # Hugging Face SpacesëŠ” 7860 í¬íŠ¸ ì‚¬ìš©
        log_level="info"
    )

