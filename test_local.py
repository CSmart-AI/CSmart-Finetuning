#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¡œì»¬ ëª¨ë¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ (ì„œë²„ ì—†ì´)
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import os

def load_local_model():
    """ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸ ì§ì ‘ ë¡œë“œ"""
    print("ğŸš€ ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    try:
        # ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸ ê²½ë¡œ í™•ì¸
        model_path = "./gemma2-finetuned"
        if not os.path.exists(model_path):
            print(f"âŒ íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            return None, None
        
        print(f"   ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_path}")
        
        # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (ë¡œì»¬ì—ì„œ)
        print("   ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        base_model = AutoModelForCausalLM.from_pretrained(
            "google/gemma-2b",
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # 2. LoRA adapter ë¡œë“œ
        print("   ğŸ“¥ LoRA adapter ë¡œë“œ ì¤‘...")
        model = PeftModel.from_pretrained(base_model, model_path)
        
        # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            "google/gemma-2b",
            trust_remote_code=True
        )
        
        model.eval()
        print(f"âœ… íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (Device: {next(model.parameters()).device})")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def ask_question(model, tokenizer, question, max_tokens=120, temperature=0.3):
    """ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±"""
    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"ì§ˆë¬¸: {question}\në‹µë³€:"
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt")
        device = next(model.parameters()).device
        inputs = inputs.to(device)
        
        # ë‹µë³€ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_k=40,
                top_p=0.8,
                do_sample=True,
                repetition_penalty=1.8,
                no_repeat_ngram_size=4,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        answer_only = full_output
        if "ë‹µë³€:" in full_output:
            answer_only = full_output.split("ë‹µë³€:", 1)[1].strip()
            
            # ë‹¤ìŒ ì§ˆë¬¸ì´ ë‚˜ì˜¤ë©´ ê·¸ ì „ê¹Œì§€ë§Œ ì¶”ì¶œ
            if "ì§ˆë¬¸:" in answer_only:
                answer_only = answer_only.split("ì§ˆë¬¸:", 1)[0].strip()
            
            # HTML íƒœê·¸ë‚˜ ì´ìƒí•œ ë¬¸ì ì œê±°
            if "<" in answer_only:
                answer_only = answer_only.split("<", 1)[0].strip()
        
        return answer_only, full_output
        
    except Exception as e:
        return f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}", ""

def test_local_model():
    """ë¡œì»¬ ëª¨ë¸ ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    print("="*60)
    print("ğŸ§ª ë¡œì»¬ ëª¨ë¸ ì§ì ‘ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # 1. ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_local_model()
    if model is None or tokenizer is None:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return False
    
    # 2. ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸...")
    test_questions = [
        "ì˜¤ë‹µë…¸íŠ¸ëŠ” ì–´ë–»ê²Œ ì •ë¦¬í• ê¹Œìš”?",
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n--- í…ŒìŠ¤íŠ¸ {i}/{len(test_questions)} ---")
        print(f"ğŸ¤” ì§ˆë¬¸: {question}")
        
        print("â³ ë‹µë³€ ìƒì„± ì¤‘...")
        answer, full_output = ask_question(model, tokenizer, question)
        
        if answer.startswith("âŒ"):
            print(f"âŒ {answer}")
        else:
            print(f"âœ… ë‹µë³€: {answer}")
            print(f"ğŸ“Š Device: {next(model.parameters()).device}")
    
    print("\n" + "="*60)
    print("ğŸ‰ ë¡œì»¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
    return True

if __name__ == "__main__":
    test_local_model()
